{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1zXaEP3K67BHYqMNWoOVu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidzangiabadi/DQN-ACO/blob/main/DQNandACO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "O3Vh5K2NrbgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "class FeatureSelectionEnv:\n",
        "    def __init__(self, X, y, penalty_factor=0.01):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n_features = X.shape[1]\n",
        "        self.state = np.zeros(self.n_features)  # Binary state vector indicating feature selection\n",
        "        self.current_step = 0\n",
        "        self.penalty_factor = penalty_factor  # Factor to penalize feature count\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(self.n_features)  # Reset state to all features unselected\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Ensure action is a scalar value (0 or 1)\n",
        "        action = int(action)  # Convert action to integer\n",
        "\n",
        "        if action not in [0, 1]:\n",
        "            raise ValueError(\"Action must be either 0 or 1\")\n",
        "\n",
        "        # Apply action to state\n",
        "        self.state[self.current_step] = action\n",
        "        self.current_step += 1\n",
        "\n",
        "        done = self.current_step == self.n_features\n",
        "        reward = self.evaluate() if done else 0\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def evaluate(self):\n",
        "        selected_features = [i for i in range(self.n_features) if self.state[i] == 1]\n",
        "        if len(selected_features) == 0:\n",
        "            return 0\n",
        "        X_selected = self.X[:, selected_features]\n",
        "        classifier = RandomForestClassifier()\n",
        "        score = cross_val_score(classifier, X_selected, self.y, cv=5).mean()\n",
        "        # Penalize based on the number of features selected\n",
        "        penalty = self.penalty_factor * len(selected_features)\n",
        "        return score - penalty\n"
      ],
      "metadata": {
        "id": "adHgx-b0wLQe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_features, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)  # Output 2 values for action selection\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "9zazIGAQuG4o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "tTYMvm-uwU5t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def select_action(state, policy_net, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, 1)  # Random action (exploration)\n",
        "    with torch.no_grad():\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
        "        q_values = policy_net(state_tensor)\n",
        "        return q_values.argmax().item()  # Best action (exploitation)\n",
        "\n",
        "def optimize_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    transitions = memory.sample(batch_size)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    state_batch = torch.FloatTensor(batch.state)\n",
        "    action_batch = torch.LongTensor(batch.action).unsqueeze(1)\n",
        "    reward_batch = torch.FloatTensor(batch.reward)\n",
        "    next_state_batch = torch.FloatTensor(batch.next_state)\n",
        "\n",
        "    q_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    next_q_values = target_net(next_state_batch).max(1)[0].detach()\n",
        "    expected_q_values = reward_batch + (gamma * next_q_values)\n",
        "\n",
        "    loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "e2tKl1NuuKQV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def select_action(state, policy_net, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, 1)  # Random action (exploration)\n",
        "    with torch.no_grad():\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
        "        q_values = policy_net(state_tensor)\n",
        "        return q_values.argmax().item()  # Best action (exploitation)\n",
        "\n",
        "def optimize_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    transitions = memory.sample(batch_size)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    state_batch = torch.FloatTensor(batch.state)\n",
        "    action_batch = torch.LongTensor(batch.action).unsqueeze(1)\n",
        "    reward_batch = torch.FloatTensor(batch.reward)\n",
        "    next_state_batch = torch.FloatTensor(batch.next_state)\n",
        "\n",
        "    q_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    next_q_values = target_net(next_state_batch).max(1)[0].detach()\n",
        "    expected_q_values = reward_batch + (gamma * next_q_values)\n",
        "\n",
        "    loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Parameters\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "batch_size = 32\n",
        "target_update = 10\n",
        "n_episodes = 20\n",
        "\n",
        "# Initialize environment, DQN, and optimizer\n",
        "env = FeatureSelectionEnv(X_train, y_train, penalty_factor=0.01)  # Adjust penalty_factor as needed\n",
        "n_features = env.n_features\n",
        "\n",
        "policy_net = DQN(n_features)\n",
        "target_net = DQN(n_features)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
        "memory = ReplayMemory(1000)\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = select_action(state, policy_net, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        memory.push(state, action, reward, next_state)\n",
        "        optimize_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
        "        state = next_state\n",
        "\n",
        "    if episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode + 1} completed\")\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Extract the selected features\n",
        "selected_features = [i for i in range(n_features) if env.state[i] == 1]\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "# Evaluate the performance using the selected features\n",
        "def evaluate_selected_features(X_test, y_test, selected_features):\n",
        "    if len(selected_features) == 0:\n",
        "        return 0\n",
        "    X_selected = X_test[:, selected_features]\n",
        "    classifier = RandomForestClassifier()\n",
        "    classifier.fit(X_train[:, selected_features], y_train)\n",
        "    accuracy = classifier.score(X_selected, y_test)\n",
        "    return accuracy\n",
        "\n",
        "accuracy = evaluate_selected_features(X_test, y_test, selected_features)\n",
        "print(f\"Accuracy with selected features: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2mXARnsuMd3",
        "outputId": "046ea9f6-540e-443a-d2c6-2b06cb4f4372"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 completed\n",
            "Episode 2 completed\n",
            "Episode 3 completed\n",
            "Episode 4 completed\n",
            "Episode 5 completed\n",
            "Episode 6 completed\n",
            "Episode 7 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c9b015c5bc32>:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  state_batch = torch.FloatTensor(batch.state)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 8 completed\n",
            "Episode 9 completed\n",
            "Episode 10 completed\n",
            "Episode 11 completed\n",
            "Episode 12 completed\n",
            "Episode 13 completed\n",
            "Episode 14 completed\n",
            "Episode 15 completed\n",
            "Episode 16 completed\n",
            "Episode 17 completed\n",
            "Episode 18 completed\n",
            "Episode 19 completed\n",
            "Episode 20 completed\n",
            "Training completed.\n",
            "Selected features: [0, 1]\n",
            "Accuracy with selected features: 0.7778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UH5EtLX2wr1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}